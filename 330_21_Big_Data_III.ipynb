{"cells": [{"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "import re", "execution_count": 1, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "7ed0ee3e3be643548ea0c6e55f428b07"}}, "metadata": {}}, {"output_type": "stream", "text": "Starting Spark application\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1554783662029_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-80-78.ec2.internal:20888/proxy/application_1554783662029_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-80-78.ec2.internal:8042/node/containerlogs/container_1554783662029_0001_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "text": "SparkSession available as 'spark'.\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "input_file = sc.textFile('s3://umsi-data-science/data/totc.txt')", "execution_count": 2, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5adc4e61724c496b92edce32fb947b43"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "WORD_RE = re.compile(r\"\\b[\\w']+\\b\")", "execution_count": 3, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c18bb276577c43d38bc277ffcc20e7ee"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "input_file.take(2)", "execution_count": 4, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "20ff3b517fef45eebef9f8d420d8b319"}}, "metadata": {}}, {"output_type": "stream", "text": "[u'The Project Gutenberg EBook of A Tale of Two Cities, by Charles Dickens', u'']", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "word_count1 = input_file.flatMap(lambda line: WORD_RE.findall(line+' '))\nword_count2 = word_count1.map(lambda word: (word, 1))\nword_count3 = word_count2.reduceByKey(lambda a, b: a + b)", "execution_count": 13, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "64e485989970474db3bdbcd8ac3e8f4d"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "type(input_file)", "execution_count": 14, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "cb6ca59a087c43af9aa98b2f4784f0f3"}}, "metadata": {}}, {"output_type": "stream", "text": "<class 'pyspark.rdd.RDD'>", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "word_counts_sorted = word_count3.sortBy(lambda x: x[1], ascending =\nFalse)", "execution_count": 15, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0e6b3c6f4b7342b4b0ff79bf9d433862"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "common_words = word_counts_sorted.take(5)", "execution_count": 11, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "730599d07c2845b8bd83a23498fa110c"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "for word in common_words:\n    print(word[0],word[1])", "execution_count": 12, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "3a224d649baa409f82db6c3a36f3f2d7"}}, "metadata": {}}, {"output_type": "stream", "text": "(u'the', 7577)\n(u'and', 4921)\n(u'of', 4102)\n(u'to', 3601)\n(u'a', 2864)", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "input_file = sc.textFile('s3://umsi-data-science/data/totc.txt')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Day 2"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df_from_other_list = spark.createDataFrame([('Chris',67),('Logan',70)], ['name','score'])\ndf_from_other_list.show()", "execution_count": 40, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "00709e45986540c7ae38470f4d3a8c3e"}}, "metadata": {}}, {"output_type": "stream", "text": "+-----+-----+\n| name|score|\n+-----+-----+\n|Chris|   67|\n|Logan|   70|\n+-----+-----+", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "from pyspark.sql.types import FloatType\ndf_from_list = spark.createDataFrame([1.0,2.0,3.0,4.0,5.0], FloatType())\ndf_from_list.show()", "execution_count": 39, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "d9de9381329742ebb607b83b73cb835e"}}, "metadata": {}}, {"output_type": "stream", "text": "+-----+\n|value|\n+-----+\n|  1.0|\n|  2.0|\n|  3.0|\n|  4.0|\n|  5.0|\n+-----+", "name": "stdout"}]}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "lot_rdd = sc.parallelize([('Chris',67),('Logan',70)])", "execution_count": 41, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "37a7dd3f7b6c4d14b49bab544470bb1d"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "dfPeople = spark.createDataFrame(lot_rdd)\ndfPeople.show()", "execution_count": 42, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "9b698bdc725a4d3f8da025c636a97a5c"}}, "metadata": {}}, {"output_type": "stream", "text": "+-----+---+\n|   _1| _2|\n+-----+---+\n|Chris| 67|\n|Logan| 70|\n+-----+---+", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "from pyspark.sql import Row\nlot_rdd_named_columns = lot_rdd.map(lambda x: Row(name=x[0], score=int(x[1])))\ndfPeople_named_columns = spark.createDataFrame(lot_rdd_named_columns)\ndfPeople_named_columns.show()", "execution_count": 45, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "7031ca88c57a4cbe8702cda4e6479401"}}, "metadata": {}}, {"output_type": "stream", "text": "+-----+-----+\n| name|score|\n+-----+-----+\n|Chris|   67|\n|Logan|   70|\n+-----+-----+", "name": "stdout"}]}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "df = spark.read.json('s3://umsi-data-science/data/yelp/business.json')", "execution_count": 2, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "89efea65f9c44e28a2aa160539970c5a"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df.show()", "execution_count": 3, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "bbf270040d5e4b0595c8c565ccb0b0f4"}}, "metadata": {}}, {"output_type": "stream", "text": "+--------------------+--------------------+--------------------+--------------------+--------------+--------------------+-------+-------------+--------------+--------------------+------------------+-----------+------------+-----+-----+\n|             address|          attributes|         business_id|          categories|          city|               hours|is_open|     latitude|     longitude|                name|      neighborhood|postal_code|review_count|stars|state|\n+--------------------+--------------------+--------------------+--------------------+--------------+--------------------+-------+-------------+--------------+--------------------+------------------+-----------+------------+-----+-----+\n|4855 E Warner Rd,...|[true,,,,,,,,, tr...|FYWN1wneV18bWNgQj...|[Dentists, Genera...|     Ahwatukee|[7:30-17:00, 7:30...|      1|   33.3306902|  -111.9785992|    Dental by Design|                  |      85044|          22|  4.0|   AZ|\n|  3101 Washington Rd|[,,,,,,,,, true, ...|He-G7vWjzVUysIKrf...|[Hair Stylists, H...|      McMurray|[9:00-16:00, 9:00...|      1|   40.2916853|   -80.1048999| Stephen Szabo Salon|                  |      15317|          11|  3.0|   PA|\n|6025 N 27th Ave, ...|[,,,,,,,,,,,,,,,,...|KQPW8lFf1y5BT2Mxi...|[Departments of M...|       Phoenix|            [,,,,,,]|      1|   33.5249025|  -112.1153098|Western Motor Veh...|                  |      85017|          18|  1.5|   AZ|\n|5000 Arizona Mill...|[,,,,,,, true,, t...|8DShNS-LuFqpEWIp0...|[Sporting Goods, ...|         Tempe|[10:00-21:00, 10:...|      0|   33.3831468|  -111.9647254|    Sports Authority|                  |      85282|           9|  3.0|   AZ|\n|        581 Howe Ave|[,, full_bar, [tr...|PfOCPjBrlQAnz__NX...|[American (New), ...|Cuyahoga Falls|[11:00-1:00, 11:0...|      1|   41.1195346|   -81.4756898|Brick House Taver...|                  |      44221|         116|  3.5|   OH|\n|      Richterstr. 11|[,, beer_and_wine...|o9eMRCWt5PkpLDE0g...|[Italian, Restaur...|     Stuttgart|[18:00-0:00, 18:0...|      1|      48.7272|       9.14795|             Messina|                  |      70567|           5|  4.0|   BW|\n|2620 Regatta Dr, ...|[,,,,,,,,, false,...|kCoE3jvEtg6UVz5SO...|[Real Estate Serv...|     Las Vegas|[8:00-17:00, 8:00...|      1|     36.20743|    -115.26846|          BDJ Realty|         Summerlin|      89128|           5|  4.0|   NV|\n|7240 W Lake Mead ...|[,,,,,,, true,, t...|OD2hnuuTJI9uotcKy...|[Shopping, Sporti...|     Las Vegas|[11:00-19:00, 11:...|      1|   36.1974844|  -115.2496601|         Soccer Zone|                  |      89128|           9|  1.5|   NV|\n|2612 Brandt Schoo...|[,,,,,,, false,, ...|EsMcGiZaQuG1OOvL9...|[Coffee & Tea, Ic...|       Wexford|            [,,,,,,]|      1|40.6151022445|-80.0913487465|    Any Given Sundae|                  |      15090|          15|  5.0|   PA|\n|                    |[,,,,,,,,, true,,...|TGWhGNusxyMaA4kQV...|[Automotive, Auto...|     Henderson|[9:00-18:00, 9:00...|      1|36.0558252127| -115.04635039|Detailing Gone Mo...|                  |      89014|           7|  5.0|   NV|\n|    737 West Pike St|[,, none,,,,, tru...|XOSRcvtaKc_Q5H1SA...|[Breakfast & Brun...|       Houston|            [,,,,,,]|      0|40.2415480142|-80.2128151059|   East Coast Coffee|                  |      15342|           3|  4.5|   PA|\n|2414 South Gilber...|[,,,,,,,,, true,,...|Y0eMNa5C-YU1RQOZf...|[Local Services, ...|      Chandler|[9:30-18:00, 9:30...|      1|   33.2717201|  -111.7912569|CubeSmart Self St...|                  |      85286|          23|  5.0|   AZ|\n|    35 Main Street N|[,,,,,,, true,, f...|xcgFnd-MwkZeO5G2H...|[Bakeries, Bagels...|       Markham|            [,,,,,,]|      1|   43.8751774|   -79.2601532|T & T Bakery and ...|   Markham Village|    L3P 1X3|          38|  4.0|   ON|\n|    107 Whitaker Str|[true,,,,,,,,, tr...|NmZtoE3v8RdSJEczY...|[General Dentistr...|     Homestead|[8:00-17:00, 8:00...|      1|   40.4014882|   -79.8879161|Complete Dental Care|                  |      15120|           5|  2.0|   PA|\n|        600 E 4th St|[,,, [false, fals...|fNMVV_ZX7CJSDWQGd...|[Restaurants, Ame...|     Charlotte|[7:00-15:00, 7:00...|      1|   35.2216474|   -80.8393449|Showmars Governme...|            Uptown|      28202|           7|  3.5|   NC|\n|       2459 Yonge St|[,, full_bar, [fa...|l09JfMeQ6ynYs5MCJ...|[Italian, French,...|       Toronto|[9:00-22:00, 9:00...|      0|   43.7113993|   -79.3993388|      Alize Catering|Yonge and Eglinton|    M4P 2H6|          12|  3.0|   ON|\n|8411 W Thunderbir...|[,,,,,,, false,, ...|IQSlT5jGE6CCDhSG0...|[Beauty & Spas, N...|        Peoria|[9:00-19:00, 9:00...|      1|   33.6086538|  -112.2400118|      T & Y Nail Spa|                  |      85381|          20|  3.0|   AZ|\n|    2518 Ironwood Dr|[,,,,,,,,, true,,...|b2I2DXtZVnpUMCXp1...|[Tires, Oil Chang...|   Sun Prairie|[7:30-18:00, 7:30...|      1|     43.18508|    -89.262047|Meineke Car Care ...|                  |      53590|           9|  3.5|   WI|\n|    13375 W McDowell|[,,,,,,, true,, t...|0FMKDOU8TJT1x87OK...|[Barbers, Beauty ...|      Goodyear|[9:00-19:00, 9:00...|      1|    33.463629|   -112.347038|Senior's Barber Shop|                  |      85395|          65|  5.0|   AZ|\n|9665 Bayview Aven...|[,, full_bar, [tr...|Gu-xs3NIQTj3Mj2xY...|[French, Food, Ba...| Richmond Hill|[11:30-23:00, 11:...|      1|   43.8675648|   -79.4126618|Maxim Bakery & Re...|                  |    L4C 9V4|          34|  3.5|   ON|\n+--------------------+--------------------+--------------------+--------------------+--------------+--------------------+-------+-------------+--------------+--------------------+------------------+-----------+------------+-----+-----+\nonly showing top 20 rows", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df.printSchema()", "execution_count": 4, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "01507fdf9f384af7bed638ea10107792"}}, "metadata": {}}, {"output_type": "stream", "text": "root\n |-- address: string (nullable = true)\n |-- attributes: struct (nullable = true)\n |    |-- AcceptsInsurance: boolean (nullable = true)\n |    |-- AgesAllowed: string (nullable = true)\n |    |-- Alcohol: string (nullable = true)\n |    |-- Ambience: struct (nullable = true)\n |    |    |-- casual: boolean (nullable = true)\n |    |    |-- classy: boolean (nullable = true)\n |    |    |-- divey: boolean (nullable = true)\n |    |    |-- hipster: boolean (nullable = true)\n |    |    |-- intimate: boolean (nullable = true)\n |    |    |-- romantic: boolean (nullable = true)\n |    |    |-- touristy: boolean (nullable = true)\n |    |    |-- trendy: boolean (nullable = true)\n |    |    |-- upscale: boolean (nullable = true)\n |    |-- BYOB: boolean (nullable = true)\n |    |-- BYOBCorkage: string (nullable = true)\n |    |-- BestNights: struct (nullable = true)\n |    |    |-- friday: boolean (nullable = true)\n |    |    |-- monday: boolean (nullable = true)\n |    |    |-- saturday: boolean (nullable = true)\n |    |    |-- sunday: boolean (nullable = true)\n |    |    |-- thursday: boolean (nullable = true)\n |    |    |-- tuesday: boolean (nullable = true)\n |    |    |-- wednesday: boolean (nullable = true)\n |    |-- BikeParking: boolean (nullable = true)\n |    |-- BusinessAcceptsBitcoin: boolean (nullable = true)\n |    |-- BusinessAcceptsCreditCards: boolean (nullable = true)\n |    |-- BusinessParking: struct (nullable = true)\n |    |    |-- garage: boolean (nullable = true)\n |    |    |-- lot: boolean (nullable = true)\n |    |    |-- street: boolean (nullable = true)\n |    |    |-- valet: boolean (nullable = true)\n |    |    |-- validated: boolean (nullable = true)\n |    |-- ByAppointmentOnly: boolean (nullable = true)\n |    |-- Caters: boolean (nullable = true)\n |    |-- CoatCheck: boolean (nullable = true)\n |    |-- Corkage: boolean (nullable = true)\n |    |-- DietaryRestrictions: struct (nullable = true)\n |    |    |-- dairy-free: boolean (nullable = true)\n |    |    |-- gluten-free: boolean (nullable = true)\n |    |    |-- halal: boolean (nullable = true)\n |    |    |-- kosher: boolean (nullable = true)\n |    |    |-- soy-free: boolean (nullable = true)\n |    |    |-- vegan: boolean (nullable = true)\n |    |    |-- vegetarian: boolean (nullable = true)\n |    |-- DogsAllowed: boolean (nullable = true)\n |    |-- DriveThru: boolean (nullable = true)\n |    |-- GoodForDancing: boolean (nullable = true)\n |    |-- GoodForKids: boolean (nullable = true)\n |    |-- GoodForMeal: struct (nullable = true)\n |    |    |-- breakfast: boolean (nullable = true)\n |    |    |-- brunch: boolean (nullable = true)\n |    |    |-- dessert: boolean (nullable = true)\n |    |    |-- dinner: boolean (nullable = true)\n |    |    |-- latenight: boolean (nullable = true)\n |    |    |-- lunch: boolean (nullable = true)\n |    |-- HairSpecializesIn: struct (nullable = true)\n |    |    |-- africanamerican: boolean (nullable = true)\n |    |    |-- asian: boolean (nullable = true)\n |    |    |-- coloring: boolean (nullable = true)\n |    |    |-- curly: boolean (nullable = true)\n |    |    |-- extensions: boolean (nullable = true)\n |    |    |-- kids: boolean (nullable = true)\n |    |    |-- perms: boolean (nullable = true)\n |    |    |-- straightperms: boolean (nullable = true)\n |    |-- HappyHour: boolean (nullable = true)\n |    |-- HasTV: boolean (nullable = true)\n |    |-- Music: struct (nullable = true)\n |    |    |-- background_music: boolean (nullable = true)\n |    |    |-- dj: boolean (nullable = true)\n |    |    |-- jukebox: boolean (nullable = true)\n |    |    |-- karaoke: boolean (nullable = true)\n |    |    |-- live: boolean (nullable = true)\n |    |    |-- no_music: boolean (nullable = true)\n |    |    |-- video: boolean (nullable = true)\n |    |-- NoiseLevel: string (nullable = true)\n |    |-- Open24Hours: boolean (nullable = true)\n |    |-- OutdoorSeating: boolean (nullable = true)\n |    |-- RestaurantsAttire: string (nullable = true)\n |    |-- RestaurantsCounterService: boolean (nullable = true)\n |    |-- RestaurantsDelivery: boolean (nullable = true)\n |    |-- RestaurantsGoodForGroups: boolean (nullable = true)\n |    |-- RestaurantsPriceRange2: long (nullable = true)\n |    |-- RestaurantsReservations: boolean (nullable = true)\n |    |-- RestaurantsTableService: boolean (nullable = true)\n |    |-- RestaurantsTakeOut: boolean (nullable = true)\n |    |-- Smoking: string (nullable = true)\n |    |-- WheelchairAccessible: boolean (nullable = true)\n |    |-- WiFi: string (nullable = true)\n |-- business_id: string (nullable = true)\n |-- categories: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- city: string (nullable = true)\n |-- hours: struct (nullable = true)\n |    |-- Friday: string (nullable = true)\n |    |-- Monday: string (nullable = true)\n |    |-- Saturday: string (nullable = true)\n |    |-- Sunday: string (nullable = true)\n |    |-- Thursday: string (nullable = true)\n |    |-- Tuesday: string (nullable = true)\n |    |-- Wednesday: string (nullable = true)\n |-- is_open: long (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- name: string (nullable = true)\n |-- neighborhood: string (nullable = true)\n |-- postal_code: string (nullable = true)\n |-- review_count: long (nullable = true)\n |-- stars: double (nullable = true)\n |-- state: string (nullable = true)", "name": "stdout"}]}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "df.select(\"name\").show()", "execution_count": 5, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f29efe90c8f34424897118a2bcf692ec"}}, "metadata": {}}, {"output_type": "stream", "text": "+--------------------+\n|                name|\n+--------------------+\n|    Dental by Design|\n| Stephen Szabo Salon|\n|Western Motor Veh...|\n|    Sports Authority|\n|Brick House Taver...|\n|             Messina|\n|          BDJ Realty|\n|         Soccer Zone|\n|    Any Given Sundae|\n|Detailing Gone Mo...|\n|   East Coast Coffee|\n|CubeSmart Self St...|\n|T & T Bakery and ...|\n|Complete Dental Care|\n|Showmars Governme...|\n|      Alize Catering|\n|      T & Y Nail Spa|\n|Meineke Car Care ...|\n|Senior's Barber Shop|\n|Maxim Bakery & Re...|\n+--------------------+\nonly showing top 20 rows", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df.select(df['name'], df['review_count'] + 1).show()", "execution_count": 6, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "9d22e053acf1477394eed8d210a9db63"}}, "metadata": {}}, {"output_type": "stream", "text": "+--------------------+------------------+\n|                name|(review_count + 1)|\n+--------------------+------------------+\n|    Dental by Design|                23|\n| Stephen Szabo Salon|                12|\n|Western Motor Veh...|                19|\n|    Sports Authority|                10|\n|Brick House Taver...|               117|\n|             Messina|                 6|\n|          BDJ Realty|                 6|\n|         Soccer Zone|                10|\n|    Any Given Sundae|                16|\n|Detailing Gone Mo...|                 8|\n|   East Coast Coffee|                 4|\n|CubeSmart Self St...|                24|\n|T & T Bakery and ...|                39|\n|Complete Dental Care|                 6|\n|Showmars Governme...|                 8|\n|      Alize Catering|                13|\n|      T & Y Nail Spa|                21|\n|Meineke Car Care ...|                10|\n|Senior's Barber Shop|                66|\n|Maxim Bakery & Re...|                35|\n+--------------------+------------------+\nonly showing top 20 rows", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df.filter(df['stars'] >= 4).show()", "execution_count": 7, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b9b09d91bae74b5b9d366e5e70151216"}}, "metadata": {}}, {"output_type": "stream", "text": "+--------------------+--------------------+--------------------+--------------------+------------+--------------------+-------+-------------+--------------+--------------------+---------------+-----------+------------+-----+-----+\n|             address|          attributes|         business_id|          categories|        city|               hours|is_open|     latitude|     longitude|                name|   neighborhood|postal_code|review_count|stars|state|\n+--------------------+--------------------+--------------------+--------------------+------------+--------------------+-------+-------------+--------------+--------------------+---------------+-----------+------------+-----+-----+\n|4855 E Warner Rd,...|[true,,,,,,,,, tr...|FYWN1wneV18bWNgQj...|[Dentists, Genera...|   Ahwatukee|[7:30-17:00, 7:30...|      1|   33.3306902|  -111.9785992|    Dental by Design|               |      85044|          22|  4.0|   AZ|\n|      Richterstr. 11|[,, beer_and_wine...|o9eMRCWt5PkpLDE0g...|[Italian, Restaur...|   Stuttgart|[18:00-0:00, 18:0...|      1|      48.7272|       9.14795|             Messina|               |      70567|           5|  4.0|   BW|\n|2620 Regatta Dr, ...|[,,,,,,,,, false,...|kCoE3jvEtg6UVz5SO...|[Real Estate Serv...|   Las Vegas|[8:00-17:00, 8:00...|      1|     36.20743|    -115.26846|          BDJ Realty|      Summerlin|      89128|           5|  4.0|   NV|\n|2612 Brandt Schoo...|[,,,,,,, false,, ...|EsMcGiZaQuG1OOvL9...|[Coffee & Tea, Ic...|     Wexford|            [,,,,,,]|      1|40.6151022445|-80.0913487465|    Any Given Sundae|               |      15090|          15|  5.0|   PA|\n|                    |[,,,,,,,,, true,,...|TGWhGNusxyMaA4kQV...|[Automotive, Auto...|   Henderson|[9:00-18:00, 9:00...|      1|36.0558252127| -115.04635039|Detailing Gone Mo...|               |      89014|           7|  5.0|   NV|\n|    737 West Pike St|[,, none,,,,, tru...|XOSRcvtaKc_Q5H1SA...|[Breakfast & Brun...|     Houston|            [,,,,,,]|      0|40.2415480142|-80.2128151059|   East Coast Coffee|               |      15342|           3|  4.5|   PA|\n|2414 South Gilber...|[,,,,,,,,, true,,...|Y0eMNa5C-YU1RQOZf...|[Local Services, ...|    Chandler|[9:30-18:00, 9:30...|      1|   33.2717201|  -111.7912569|CubeSmart Self St...|               |      85286|          23|  5.0|   AZ|\n|    35 Main Street N|[,,,,,,, true,, f...|xcgFnd-MwkZeO5G2H...|[Bakeries, Bagels...|     Markham|            [,,,,,,]|      1|   43.8751774|   -79.2601532|T & T Bakery and ...|Markham Village|    L3P 1X3|          38|  4.0|   ON|\n|    13375 W McDowell|[,,,,,,, true,, t...|0FMKDOU8TJT1x87OK...|[Barbers, Beauty ...|    Goodyear|[9:00-19:00, 9:00...|      1|    33.463629|   -112.347038|Senior's Barber Shop|               |      85395|          65|  5.0|   AZ|\n|     85 Hanna Avenue|[,,,,,,, true,, t...|lHYiCS-y8AFjUitv6...|[Food, Coffee & Tea]|     Toronto|[5:30-23:00, 5:30...|      1|43.6398633116|-79.4195331865|           Starbucks|Liberty Village|    M6K 3S3|          21|  4.0|   ON|\n|   1928 S Gilbert Rd|[,,,,,,, false,, ...|94KziT6DQ9XlBET3W...|[Shopping, Books,...|        Mesa|            [,,,,,,]|      0|   33.3801334|  -111.7887679|                 Fye|               |      85204|           4|  4.5|   AZ|\n|5775 S Eastern, S...|[,,,,,,, true,, t...|VBHEsoXQb2AQ76J9l...|[Shopping, Jewelr...|   Las Vegas|[10:00-17:00, 10:...|      1|36.0850508374|-115.119420802|   Alfredo's Jewelry|      Southeast|      89119|          23|  4.5|   NV|\n|  10 Dundas Street E|[,,,,,,, true,, t...|AtdXq_gu9NTE5rx4c...|[Coffee & Tea, Fo...|     Toronto|[10:00-21:00, 10:...|      1|   43.6567287|   -79.3807182|           DAVIDsTEA|        Ryerson|    M5B 2G9|           6|  4.0|   ON|\n|2777 Steeles Aven...|[,, none,,,,, tru...|nbhBRhZtdaZmMMeb2...|[Restaurants, Bre...|     Toronto|[7:00-15:00, 7:00...|      1|   43.7818155|   -79.4904331|     Sunnyside Grill|               |    M3J 3K5|           3|  5.0|   ON|\n|12614 N Cave Cree...|[,,,,,,,,, true,,...|zzMu-6SmqhpvHxVRM...|[Oil Change Stati...|     Phoenix|[7:00-17:30, 7:00...|      1|   33.6008771|  -112.0376849|Good Brakes Autom...|               |      85022|           5|  4.0|   AZ|\n|  201 Harbord Street|[,,,,,,, true,, t...|FXHfcFVEfI1vVngW2...|[Coffee & Tea, Re...|     Toronto|[12:00-1:30, 17:0...|      1|43.6615816807|-79.4088784561|Bampot House of T...|               |    M5S 1H6|          55|  4.0|   ON|\n|  8439 Charlotte Hwy|[,,,,,,, true,, t...|7gquCdaFoHZCcLYDt...|[Nail Salons, Bea...|   Fort Mill|[9:30-19:30, 9:30...|      1|34.9570617825|-80.8539120108|        Bubbly Nails|               |      29707|          17|  4.0|   SC|\n|        96 S Main St|[,, full_bar, [fa...|8y56fOiKhtCnqaiYB...|[Nightlife, Pubs,...|Munroe Falls|            [,,,,,,]|      1|    41.136622|   -81.4392592|      Brewster's Pub|               |      44308|           4|  4.0|   OH|\n| 3245 W Florimond Rd|[,,,,,,,,, true,,...|ok38fApaT1TBEU-IH...|[Home Services, C...|     Phoenix|[9:00-18:00, 9:00...|      1|      33.8039|   -112.130405|Kool Pool Care & ...|               |      85086|           5|  5.0|   AZ|\n|2470 Paseo Verde ...|[,,,,,,, false, f...|lj0MiK5_fyv9df2tw...|[Hair Salons, Blo...|   Henderson|[9:00-17:00,, 9:0...|      1|   36.0194946|  -115.0947223|Pampered Hair Pas...|         Anthem|      89074|           3|  5.0|   NV|\n+--------------------+--------------------+--------------------+--------------------+------------+--------------------+-------+-------------+--------------+--------------------+---------------+-----------+------------+-----+-----+\nonly showing top 20 rows", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df.groupBy('stars').count().show()", "execution_count": 8, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "1a4f0ffa79c24b96b4b5ef3437397b1e"}}, "metadata": {}}, {"output_type": "stream", "text": "+-----+-----+\n|stars|count|\n+-----+-----+\n|  3.5|32038|\n|  4.5|24796|\n|  2.5|16148|\n|  1.0| 3788|\n|  4.0|33492|\n|  3.0|23142|\n|  2.0| 9320|\n|  1.5| 4303|\n|  5.0|27540|\n+-----+-----+", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df.groupBy('stars').count().sort('stars', ascending=False).show()", "execution_count": 9, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "6255c65745c2493fa6aff5651c2e002e"}}, "metadata": {}}, {"output_type": "stream", "text": "+-----+-----+\n|stars|count|\n+-----+-----+\n|  5.0|27540|\n|  4.5|24796|\n|  4.0|33492|\n|  3.5|32038|\n|  3.0|23142|\n|  2.5|16148|\n|  2.0| 9320|\n|  1.5| 4303|\n|  1.0| 3788|\n+-----+-----+", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df_from_other_list = spark.createDataFrame([('Chris',[67,42]),('Logan',[70,72])],['name','scores'])", "execution_count": 13, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c220b9f669c54e65b37aad502f37eae2"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df_from_other_list.show()", "execution_count": 14, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8b6e468bec9d4695bc3337d13deb463d"}}, "metadata": {}}, {"output_type": "stream", "text": "+-----+--------+\n| name|  scores|\n+-----+--------+\n|Chris|[67, 42]|\n|Logan|[70, 72]|\n+-----+--------+", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "from pyspark.sql.functions import explode", "execution_count": 15, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "ca9cf088e0f1493ba72b6be222047d36"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df_exploded = df_from_other_list.withColumn('score',explode('scores'))", "execution_count": 16, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "53b9122c8b444dd79d831302454abe06"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df_exploded.show()", "execution_count": 17, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "6499fb7a2ebd4992924bdd11ce318f27"}}, "metadata": {}}, {"output_type": "stream", "text": "+-----+--------+-----+\n| name|  scores|score|\n+-----+--------+-----+\n|Chris|[67, 42]|   67|\n|Chris|[67, 42]|   42|\n|Logan|[70, 72]|   70|\n|Logan|[70, 72]|   72|\n+-----+--------+-----+", "name": "stdout"}]}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "import pyspark.sql.functions as F\nfrom pyspark.sql.functions import col\ndf_exploded.withColumn('good',F.when(df_exploded['score'] > 50,1).otherwise(0)).show()", "execution_count": 18, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "ed7a02f1e05f43d78f759cc97682dc18"}}, "metadata": {}}, {"output_type": "stream", "text": "+-----+--------+-----+----+\n| name|  scores|score|good|\n+-----+--------+-----+----+\n|Chris|[67, 42]|   67|   1|\n|Chris|[67, 42]|   42|   0|\n|Logan|[70, 72]|   70|   1|\n|Logan|[70, 72]|   72|   1|\n+-----+--------+-----+----+", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df = spark.read.json('s3://umsi-data-science/data/yelp/review.json.gz')", "execution_count": 29, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "3b3e394036d64c79ba1845c2ed95dc3d"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "impo", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df.printSchema()", "execution_count": 30, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "d91f4e8c911a40a29a4d83101c69aba3"}}, "metadata": {}}, {"output_type": "stream", "text": "root\n |-- business_id: string (nullable = true)\n |-- cool: long (nullable = true)\n |-- date: string (nullable = true)\n |-- funny: long (nullable = true)\n |-- review_id: string (nullable = true)\n |-- stars: long (nullable = true)\n |-- text: string (nullable = true)\n |-- useful: long (nullable = true)\n |-- user_id: string (nullable = true)", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "positive_reviews = df.filter(df.stars>4).sample(False,0.001)\nnegative_reviews = df.filter(df.stars<2).sample(False,0.001)", "execution_count": 31, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "9485b171a35a486094ae46de77b4bb06"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "pos_review_words = positive_reviews.rdd.map(lambda x: x.text.lower()) \\\n    .flatMap(lambda x: nltk.regexp_tokenize(x,'\\w+')) \\\n    .filter(lambda x: x not in sw) \\\n    .map(lambda x: (x,1)) \\\n    .reduceByKey(lambda x,y: x+y) \\\n    .sortBy(lambda x: x[1], ascending=False)", "execution_count": 32, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "3462cb643aff4493857f550da99cc728"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "neg_review_words = negative_reviews.rdd.map(lambda x: x.text.lower()) \\\n    .flatMap(lambda x: nltk.regexp_tokenize(x,'\\w+')) \\\n    .filter(lambda x: x not in sw) \\\n    .map(lambda x: (x,1)) \\\n    .reduceByKey(lambda x,y: x+y) \\\n    .sortBy(lambda x: x[1], ascending=False)", "execution_count": 33, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "bf596d7df7874a498cf6e4becc06fe13"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "pos_reviews_list = pos_review_words.map(lambda x: x[0]).collect()\nneg_reviews_list = neg_review_words.map(lambda x: x[0]).collect()", "execution_count": 34, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "43bf8db115d449bcaff6de3308484633"}}, "metadata": {}}, {"output_type": "stream", "text": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 4 times, most recent failure: Lost task 0.3 in stage 26.0 (TID 422, ip-172-31-80-78.ec2.internal, executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1861, in combineLocally\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 2, in <lambda>\nNameError: global name 'nltk' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1861, in combineLocally\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 2, in <lambda>\nNameError: global name 'nltk' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 816, in collect\n    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 4 times, most recent failure: Lost task 0.3 in stage 26.0 (TID 422, ip-172-31-80-78.ec2.internal, executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1861, in combineLocally\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 2, in <lambda>\nNameError: global name 'nltk' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1861, in combineLocally\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 2, in <lambda>\nNameError: global name 'nltk' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "pos_review_words_filtered = pos_review_words.filter(lambda x: x[0] not in neg_reviews_list)\npos_review_words_filtered.take(50)", "execution_count": 35, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "bbb6ea0e58a445dcaed51ee1572e6d25"}}, "metadata": {}}, {"output_type": "stream", "text": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 28.0 failed 4 times, most recent failure: Lost task 0.3 in stage 28.0 (TID 426, ip-172-31-80-78.ec2.internal, executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1861, in combineLocally\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 2, in <lambda>\nNameError: global name 'nltk' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1861, in combineLocally\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 2, in <lambda>\nNameError: global name 'nltk' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1360, in take\n    res = self.context.runJob(self, takeUpToNumLeft, p)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py\", line 1051, in runJob\n    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 28.0 failed 4 times, most recent failure: Lost task 0.3 in stage 28.0 (TID 426, ip-172-31-80-78.ec2.internal, executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1861, in combineLocally\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 2, in <lambda>\nNameError: global name 'nltk' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1861, in combineLocally\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 2, in <lambda>\nNameError: global name 'nltk' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "neg_review_words_filtered = neg_review_words.filter(lambda x: x[0] not in pos_reviews_list)\nneg_review_words_filtered.take(50)", "execution_count": 36, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0dc5e81c4e6c4888aa51c510c42ea3c5"}}, "metadata": {}}, {"output_type": "stream", "text": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30.0 failed 4 times, most recent failure: Lost task 0.3 in stage 30.0 (TID 430, ip-172-31-80-78.ec2.internal, executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1861, in combineLocally\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 2, in <lambda>\nNameError: global name 'nltk' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1861, in combineLocally\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 2, in <lambda>\nNameError: global name 'nltk' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1360, in take\n    res = self.context.runJob(self, takeUpToNumLeft, p)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py\", line 1051, in runJob\n    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30.0 failed 4 times, most recent failure: Lost task 0.3 in stage 30.0 (TID 430, ip-172-31-80-78.ec2.internal, executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1861, in combineLocally\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 2, in <lambda>\nNameError: global name 'nltk' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1861, in combineLocally\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/mnt/yarn/usercache/livy/appcache/application_1554783662029_0001/container_1554783662029_0001_01_000005/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 2, in <lambda>\nNameError: global name 'nltk' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "pysparkkernel", "display_name": "PySpark", "language": ""}, "language_info": {"name": "pyspark", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 2}, "pygments_lexer": "python2"}}, "nbformat": 4, "nbformat_minor": 2}